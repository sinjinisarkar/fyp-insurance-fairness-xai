{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13376, 57)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_model = pd.read_csv(\"../data/processed/df_young_model_ready_2024.csv\")\n",
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int64    57\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['high_risk'].value_counts(normalize=True)\n",
    "df_model.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collision_year_x            0\n",
       "number_of_casualties        0\n",
       "local_authority_district    0\n",
       "first_road_class            0\n",
       "first_road_number           0\n",
       "road_type                   0\n",
       "speed_limit                 0\n",
       "junction_detail_historic    0\n",
       "junction_detail             0\n",
       "junction_control            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leak_cols = [c for c in df_model.columns if \"severity\" in c.lower()]\n",
    "leak_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_like = [c for c in df_model.columns if c in [\n",
    "    \"collision_index\", \"collision_ref_no_x\", \"collision_ref_no_y\", \"vehicle_reference\"\n",
    "]]\n",
    "id_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high_risk\n",
       "0    9831\n",
       "1    3545\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model[\"high_risk\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (13375, 54)\n",
      "y shape: (13375,)\n",
      "A shape: (13375, 2)\n"
     ]
    }
   ],
   "source": [
    "# Always redefine after structural changes\n",
    "# Redefine target\n",
    "y = df_model[\"high_risk\"]\n",
    "\n",
    "# Redefine protected attributes (for fairness later)\n",
    "A = df_model[[\"sex_of_driver\", \"age_band_of_driver\"]]\n",
    "\n",
    "# Redefine feature matrix\n",
    "X = df_model.drop(columns=[\"high_risk\", \"sex_of_driver\", \"age_band_of_driver\"])\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"A shape:\", A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (10700, 54)\n",
      "Test: (2675, 54)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    A,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape)\n",
    "print(\"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class balance:\n",
      "high_risk\n",
      "0    0.735047\n",
      "1    0.264953\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class balance:\n",
      "high_risk\n",
      "0    0.734953\n",
      "1    0.265047\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train class balance:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest class balance:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.737196261682243\n",
      "F1 Score: 0.1267080745341615\n",
      "ROC-AUC: 0.6558073999888083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinjinisarkar/Desktop/3rd Year - CWKs/Semester 2/Individual Project/Projects/fyp-insurance-fairness-xai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Initialize model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Baseline – Initial Results\n",
    "\n",
    "### Model Performance (Unscaled Features)\n",
    "\n",
    "- **Accuracy:** 0.737  \n",
    "- **F1 Score:** 0.127  \n",
    "- **ROC–AUC:** 0.656  \n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation of Results\n",
    "\n",
    "At first glance, the model appears to perform reasonably well, with an accuracy of approximately **73.7%**. However, this must be interpreted carefully.\n",
    "\n",
    "The dataset is **class imbalanced**, with approximately:\n",
    "\n",
    "- **73% Low Risk (0)**  \n",
    "- **27% High Risk (1)**  \n",
    "\n",
    "This means that a naive model that predicts *“Low Risk” for every case* would already achieve around **73% accuracy** without learning any meaningful patterns.\n",
    "\n",
    "Therefore, the observed accuracy (0.737) is only marginally above this majority-class baseline. Accuracy alone is therefore **not an informative metric** in this setting.\n",
    "\n",
    "---\n",
    "\n",
    "### F1 Score Analysis\n",
    "\n",
    "The **F1 score (0.127)** is very low.\n",
    "\n",
    "The F1 score balances:\n",
    "\n",
    "- **Precision** (how many predicted high-risk cases were correct)\n",
    "- **Recall** (how many actual high-risk cases were successfully identified)\n",
    "\n",
    "A low F1 score indicates that the model struggles to correctly identify **high-risk drivers**, which is the minority but more critical class in this application.\n",
    "\n",
    "Since the objective of this project is to predict higher-risk outcomes, this result suggests that the baseline Logistic Regression model is not yet effectively discriminating between low- and high-risk cases.\n",
    "\n",
    "---\n",
    "\n",
    "### ROC–AUC Interpretation\n",
    "\n",
    "The **ROC–AUC score of 0.656** suggests that the model has some ability to rank high-risk cases above low-risk cases, but its discriminatory power remains modest.\n",
    "\n",
    "For reference:\n",
    "\n",
    "- 0.5 indicates random guessing  \n",
    "- ~0.65 indicates weak-to-moderate separation  \n",
    "\n",
    "This implies the model is learning some signal from the data, but not strongly enough to produce reliable classification performance.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Convergence Warning\n",
    "\n",
    "The model produced the following warning:\n",
    "\n",
    "`lbfgs failed to converge after 1000 iterations`\n",
    "\n",
    "This indicates that the optimisation algorithm did not fully converge before reaching the iteration limit. Two likely causes are:\n",
    "\n",
    "1. The features have not yet been scaled.\n",
    "2. The feature space contains variables with different magnitudes.\n",
    "\n",
    "Logistic Regression performs best when features are standardised. This warning suggests that feature scaling (e.g., using `StandardScaler`) should be applied before drawing firm conclusions about model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Baseline Conclusion\n",
    "\n",
    "This baseline experiment highlights three key points:\n",
    "\n",
    "1. Accuracy is misleading due to class imbalance.\n",
    "2. The model performs poorly at identifying high-risk cases (low F1 score).\n",
    "3. Feature scaling is required to ensure proper convergence and fair evaluation.\n",
    "\n",
    "This provides a meaningful starting benchmark against which improved preprocessing and more expressive models (e.g., Random Forest or XGBoost) can be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression (Scaled Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7357009345794393\n",
      "F1 Score: 0.10619469026548672\n",
      "ROC-AUC: 0.6627067768424285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Create pipeline: scaling + logistic regression\n",
    "lr_scaled_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train\n",
    "lr_scaled_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_scaled = lr_scaled_pipeline.predict(X_test)\n",
    "y_proba_scaled = lr_scaled_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_scaled))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_scaled))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve optimisation stability, all numerical features were standardised using a StandardScaler prior to training Logistic Regression. Scaling ensures that features with larger numeric ranges do not dominate the optimisation process.\n",
    "\n",
    "The scaled Logistic Regression model produced the following results:\n",
    "\n",
    "- Accuracy: 0.736\n",
    "- F1 Score: 0.106\n",
    "- ROC-AUC: 0.663\n",
    "\n",
    "Compared to the unscaled version, ROC-AUC improved slightly, indicating better probability ranking between high-risk and low-risk drivers. The convergence warning observed previously was resolved after scaling, suggesting improved optimisation behaviour.\n",
    "\n",
    "However, the F1 score remained low, indicating that the model still struggles to correctly identify high-risk drivers. This reflects the moderate class imbalance in the dataset (~27% high-risk vs ~73% low-risk).\n",
    "\n",
    "These results suggest that while scaling improves optimisation stability, additional techniques may be required to better handle class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.98      0.84      1966\n",
      "           1       0.51      0.06      0.11       709\n",
      "\n",
      "    accuracy                           0.74      2675\n",
      "   macro avg       0.63      0.52      0.48      2675\n",
      "weighted avg       0.68      0.74      0.65      2675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5951401869158879\n",
      "F1 Score: 0.45714285714285713\n",
      "ROC-AUC: 0.6627397779171156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Create pipeline: scaling + balanced logistic regression\n",
    "lr_balanced_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=5000, \n",
    "                                 class_weight='balanced', \n",
    "                                 random_state=42))\n",
    "])\n",
    "\n",
    "# Train\n",
    "lr_balanced_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_bal = lr_balanced_pipeline.predict(X_test)\n",
    "y_proba_bal = lr_balanced_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bal))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_bal))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.58      0.68      1966\n",
      "           1       0.35      0.64      0.46       709\n",
      "\n",
      "    accuracy                           0.60      2675\n",
      "   macro avg       0.59      0.61      0.57      2675\n",
      "weighted avg       0.70      0.60      0.62      2675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class distribution:\n",
      "[1389 1286]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Predicted class distribution:\")\n",
    "print(np.bincount(y_pred_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
